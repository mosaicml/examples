name: llama2-13b
replicas: 1
command: |- # Note this command is a workaround until we build vllm into the inference image
  pip install vllm==0.1.3
  pip uninstall torch -y
  pip install torch==2.0.1
compute:
  gpus: 2
  instance: oci.vm.gpu.a10.2
image: mosaicml/inference:0.1.40
default_model:
  model_type: llama2-13b
