# CUDA Dependencies for optimized GPT models
# Not required for default GPT blocks
# Please also see the examples/llm/csrc/ folder to install our custom CUDA optimizations.

git+https://github.com/HazyResearch/flash-attention.git@v0.2.8#subdirectory=csrc/xentropy
git+https://github.com/HazyResearch/flash-attention.git@v0.2.8#subdirectory=csrc/fused_dense_lib
