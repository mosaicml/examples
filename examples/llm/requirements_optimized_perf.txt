# CUDA Dependencies for optimized GPT models
# Not required for default GPT blocks

git+https://github.com/HazyResearch/flash-attention.git@v0.2.8#subdirectory=csrc/xentropy
git+https://github.com/HazyResearch/flash-attention.git@v0.2.8#subdirectory=csrc/fused_dense_lib
