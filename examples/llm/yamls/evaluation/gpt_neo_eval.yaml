# Tokenizer
tokenizer:
  type: hftokenizer
  args:
    tokenizer_name: gpt2
    max_seq_len: 2048

model:
  model_type: pretrained_hf
  config: EleutherAI/gpt-neo-125M

icl_tasks:
  -
    label: lambada
    dataset_uri: s3://mosaicml-internal-dataset-lambda/lambada/lambada_test.json
    num_fewshot:
      - 0
    batch_size: 8
    type: language_modeling
    metrics:
      - InContextLearningLMAccuracy
    formatting_options:
      prompt_string: ''
      example_delimiter: '\n'
      continuation_delimiter: ''
