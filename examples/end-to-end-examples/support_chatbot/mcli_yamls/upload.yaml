name: upload-support-chatbot-to-cloud

compute:
  gpus: 8  # Number of GPUs to use

cluster: r1z1 # Name of the cluster to use for this run
gpu_type: a100_80gb # Type of GPU to use

integrations:
# Clone your repository
- integration_type: git_repo
  git_repo: KuuCi/examples # TODO: CHANGE THIS TO YOUR GIT REPO URL
  git_branch: support-bot # TODO: CHANGE THIS TO YOUR BRANCH IF NEEDED
  ssh_clone: false # Should be true if using a private repo
  path: /workspace/examples # Tell MCLI what path to clone the repo to

# cd into the support_chatbot folder
# Install the necessary dependencies
# Run the script to process the raw data files and upload them to the cloud in the correct format
command: |
  cd /workspace/examples/examples/end-to-end-examples/support_chatbot
  pip install -r requirements.txt
  python upload_data.py \
    --local_file_path train_data/pipeline_testing/coqa.jsonl \
    --cloud_folder_for_upload oci://mosaic-internal-checkpoints/vincent/support_chatbot/data/support-chatbot-large/

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04 # Use the Docker image provided by MosaicML
