name: mpt-30b-composer-finetuned

# Deployment configuration
# For 30B model deployed with bf16 datatype, the model needs atleast ~60GB of memory.
# So should be hosted on atleast 2x A100-40G or 4x A10s
compute:
  gpus: 4
  instance: oci.bm.gpu.a10.4
replicas: 1
integrations:
# Clone and install the examples repo so we can use the deployment helper from it
- integration_type: git_repo
  git_repo: KuuCi/examples
  git_branch: support-bot
  ssh_clone: false
# Clone the llm-foundry repo so we can use the HF to FasterTransformer convert script from it
- git_repo: mosaicml/llm-foundry
  integration_type: git_repo
  ssh_clone: false

# Add the examples folder to the PYTHONPATH so we can import the deployment helper
# Install composer to use the cloud download helper
command: |
  export PYTHONPATH=$PYTHONPATH:/code/llm-foundry:/code/examples:/code

model:
  downloader: examples.end-to-end-examples.support_chatbot.scripts.deployment_download_helper.download_and_convert
  download_parameters:
    remote_uri: oci://mosaicml-internal-checkpoints/support-bot-demo/converted_checkpoints/mpt-30b-chat_composer_chatv2-hf/
    gpus: 4
  model_handler: examples.inference-deployments.mpt.mpt_ft_handler.MPTFTModelHandler # Use the provided MPT handler
  model_parameters:
    ft_lib_path: /code/FasterTransformer/build/lib/libth_transformer.so
    # FT checkpoint path is hardcoded in MPTFTModelHandler at /tmp/mpt
    model_name_or_path: mosaicml/mpt-30b-chat # This is used for the tokenzier
    gpus: 4 