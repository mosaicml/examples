name: mpt-7b-finance-hf-convert-final

compute:
  gpus: 8  # Number of GPUs to use

  ## These configurations are optional
  # cluster: r0z0 # Name of the cluster to use for this run
  # gpu_type: a100_80gb # Type of GPU to use.

integrations:
# Clone and install the llm-foundry repo so we can run scripts from it
- integration_type: git_repo
  git_repo: mosaicml/llm-foundry
  git_branch: main # TODO: pin
  pip_install: -e .
  ssh_clone: false # Should be true if using a private repo

# cd into the scripts/inference folder and run the MPT conversion script from LLM-foundry
command: |
  cd llm-foundry/scripts/inference
  python convert_composer_to_hf.py \
    --composer_path CLOUD://BUCKET_NAME/sec_10k_demo/checkpoints/PREVIOUS_RUN_NAME/latest-rank0.pt.symlink \
    --hf_output_path CLOUD://BUCKET_NAME/sec_10k_demo/checkpoints/mpt-7b-hf/ \
    --output_precision bf16 \

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04 # Use the Docker image provided by MosaicML
