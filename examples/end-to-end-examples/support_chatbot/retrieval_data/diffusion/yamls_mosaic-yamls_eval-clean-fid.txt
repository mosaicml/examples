run_name: diffusion-clean-fid-eval
image: mosaicml/pytorch_vision:1.13.1_cu117-python3.10-ubuntu20.04
compute:
  gpus: 8  # Number of GPUs to use. Note evaluating with clean-fid currently only supports 1 node.
  ## These configurations are optional
  # cluster: TODO # Name of the cluster to use for this run
  # gpu_type: a100_80gb # Type of GPU to use. We use a100_80gb in our experiments
integrations:
  - integration_type: "git_repo"
    git_repo: mosaicml/diffusion
    git_branch: main
    pip_install: .
  - integration_type: "wandb"
    project: # Insert wandb project name
    entity: # Insert wandb entity name

# To run eval, one must specify the path to the dataset in the remote field.
# Guidance scale can also be specified here, 3.0 is a good starting point
# load_path is the location of the checkpoint. If load_path is not specified, the pretrained model will be used.
command: |
  cd diffusion
  HYDRA_FULL_ERROR=1 composer run_eval.py --config-path /mnt/config --config-name parameters

parameters:
  image_size: 256 # This is the image resolution to evaluate at (assumes square images)
  batch_size: 16
  name: clean-fid-eval # Name for the eval run for logging
  project: diffusion-clean-fid-eval # Name of the wandb project for logging
  seed: 42 # Random seed. This affects the randomness used in image generation.

  model: # This is the model to evaluate
    _target_: diffusion.models.models.stable_diffusion_2
    pretrained: false
    precomputed_latents: false
    encode_latents_in_fp16: true
    fsdp: false
    val_metrics:
      - _target_: torchmetrics.MeanSquaredError
    val_guidance_scales: []
    loss_bins: []
  eval_dataloader:
    _target_: diffusion.datasets.build_streaming_image_caption_dataloader
    remote:
      - # Remote(s) for the evaluation dataset go here
    local:
      - # Local(s) for the evaluation dataset go here
    batch_size: ${batch_size}
    resize_size: ${image_size}
    image_key: image # This should be set to the image key specific to the eval dataset
    caption_key: captions # This should be set to the caption key specific to the eval dataset
    transform: # How to transform the images for evaluation
      - _target_ : diffusion.datasets.laion.transforms.LargestCenterSquare
        size: ${image_size}
      - _target_: torchvision.transforms.ToTensor
    dataloader_kwargs:
      drop_last: false
      shuffle: false
      num_workers: 8
      pin_memory: true
    streaming_kwargs:
      shuffle: false
  clip_metric: # This is the metric used to compute CLIP score, which is not part of clean-fid
    _target_: torchmetrics.multimodal.CLIPScore
    model_name_or_path: openai/clip-vit-base-patch16
  logger:
    wandb:
      _target_: composer.loggers.wandb_logger.WandBLogger
      name: ${name}
      project: ${project}
      group: ${name}
  evaluator:
    _target_: diffusion.evaluation.clean_fid_eval.CleanFIDEvaluator
    load_path: # Path to the checkpoint to load and evaluate.
    guidance_scales:
      - 1.0
      - 1.5
      - 2.0
      - 3.0
      - 4.0
      - 5.0
      - 6.0
      - 7.0
      - 8.0
    size: ${image_size}
    batch_size: ${batch_size}
    seed: ${seed}
