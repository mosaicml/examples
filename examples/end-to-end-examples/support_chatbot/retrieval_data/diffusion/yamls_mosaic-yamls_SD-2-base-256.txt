run_name: SD2-base-256
cluster: # Insert cluster here
gpu_num: # Insert number of GPUs
image: mosaicml/pytorch_vision:1.13.1_cu117-python3.10-ubuntu20.04
integrations:
  - integration_type: "git_repo"
    git_repo: mosaicml/diffusion2
    git_branch: main
    pip_install: .[all]
  - integration_type: "wandb"
    project: # Insert wandb project name
    entity: # Insert wandb entity name
command: |
  cd diffusion2
  HYDRA_FULL_ERROR=1 composer run.py
parameters:
  project:  # Insert wandb project name
  batch_size: 2048
  seed: 17
  scale_schedule_ratio: 1.0
  name:  # Insert wandb run name
  eval_first: false
  algorithms:
    low_precision_groupnorm:
      attribute: unet
      precision: amp_fp16
    low_precision_layernorm:
      attribute: unet
      precision: amp_fp16
  model:
    _target_: diffusion.models.models.stable_diffusion_2
    pretrained: false
    precomputed_latents: true
    encode_latents_in_fp16: true
    fsdp: true
    val_metrics:
      - _target_: torchmetrics.MeanSquaredError
    val_guidance_scales: []
    loss_bins: []
  dataset:
    train_batch_size: ${batch_size}
    eval_batch_size: 1024 # Should be 8 per device
    train_dataset:
      _target_: diffusion.datasets.laion.laion.build_streaming_laion_dataloader
      remote:
        # Path to object store bucket(s)
      local:
        # Path to corresponding local dataset(s)
      batch_size: ${batch_size}
      tokenizer_name_or_path: stabilityai/stable-diffusion-2-base
      caption_drop_prob: 0.1
      resize_size: 256
      drop_last: true
      shuffle: true
      prefetch_factor: 2
      num_workers: 8
      persistent_workers: true
      pin_memory: true
      download_timeout: 300
      num_canonical_nodes: 64
    eval_dataset:
      _target_: diffusion.datasets.coco.coco_captions.build_streaming_cocoval_dataloader
      remote: # Path to object store bucket
      local: # Path to local dataset cache
      batch_size: 8
      resize_size: 256
      prefetch_factor: 2
      num_workers: 8
      persistent_workers: True
      pin_memory: True
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1.0e-4
    weight_decay: 0.01
  scheduler:
    _target_: composer.optim.MultiStepWithWarmupScheduler
    t_warmup: 10000ba
    milestones:
      - 200ep
  logger:
    wandb:
      _target_: composer.loggers.wandb_logger.WandBLogger
      name: ${name}
      project: ${project}
      group: ${name}
  callbacks:
    speed_monitor:
      _target_: composer.callbacks.speed_monitor.SpeedMonitor
      window_size: 10
    lr_monitor:
      _target_: composer.callbacks.lr_monitor.LRMonitor
    memory_monitor:
      _target_: composer.callbacks.memory_monitor.MemoryMonitor
    runtime_estimator:
      _target_: composer.callbacks.runtime_estimator.RuntimeEstimator
    optimizer_monitor:
      _target_: composer.callbacks.OptimizerMonitor
  trainer:
    _target_: composer.Trainer
    device: gpu
    max_duration: 550000ba
    eval_interval: 10000ba
    device_train_microbatch_size: 16
    run_name: ${name}
    seed: ${seed}
    scale_schedule_ratio: ${scale_schedule_ratio}
    save_folder:  # Insert path to save folder or bucket
    save_interval: 10000ba
    save_overwrite: true
    autoresume: false
    fsdp_config:
      sharding_strategy: "SHARD_GRAD_OP"
