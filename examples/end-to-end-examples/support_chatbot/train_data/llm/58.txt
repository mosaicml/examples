Title: mismatched config compared with blog
Date: 05/2023

Question:

The blog shows that We also train our MPT models with [the Lion optimizer](https://arxiv.org/abs/2302.06675) rather than AdamW, which provides stable update magnitudes and cuts optimizer state memory in half.. But in the repo's yaml config (as follows), the AdamW is used.

llm-foundry/scripts/train/yamls/mpt/7b.yaml
Line 60 in 3959eac

optimizer:
  name: decoupled_adamw

Answer:

The configs in llm-foundry/scripts/train/yamls/mpt/are simple starter configs to get people going with our code.
They are not what we use to train the models in the blog; the model config are correct but some of the details of training were modified.