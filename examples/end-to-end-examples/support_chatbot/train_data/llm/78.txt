Title: Error in Triton implementation
Date: 05/2023

Question:

Using the below configuration i loaded the instruct model

config = transformers.AutoConfig.from_pretrained(
  'mosaicml/mpt-7b-instruct',
  trust_remote_code=True
)
config.attn_config['attn_impl'] = 'triton'

model = transformers.AutoModelForCausalLM.from_pretrained(
  'mosaicml/mpt-7b-instruct',
  config=config,
  torch_dtype=torch.bfloat16,
  trust_remote_code=True
)
model.to(device='cuda:0')
But I got error:
TypeError: dot() got an unexpected keyword argument 'trans_b'  

Answer:

That error comes up if you are not using the pinned version of triton ie it looks like your requirements are not installed correctly. Please try pip install .[gpu]
