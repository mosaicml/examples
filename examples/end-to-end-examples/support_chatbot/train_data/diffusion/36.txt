Title: MissingEnvironmentError: Torch distributed is initialized but environment variable NODE_RANK is not set
Date: 06/2023

Question:

Hi, I am trying to do the stage 2 training by loading the checkpoint from stage 1. It works on a single GPU, but failed by using 8 GPUs with the following error. Any ideas? Thanks.

Traceback (most recent call last) ────────────────────────────────╮
mosaic_512/0 [0]:│ /opt/conda/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py:92 in        │
mosaic_512/0 [0]:│ _call_target                                                                                     │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│    89 │   │   │   raise InstantiationException(msg) from e                                       │
mosaic_512/0 [0]:│    90 │   else:                                                                                  │
mosaic_512/0 [0]:│    91 │   │   try:                                                                               │
mosaic_512/0 [0]:│ ❱  92 │   │   │   return _target_(*args, **kwargs)                                               │
mosaic_512/0 [0]:│    93 │   │   except Exception as e:                                                             │
mosaic_512/0 [0]:│    94 │   │   │   msg = f"Error in call to target '{_convert_target_to_string(_target_)}':\n{r   │
mosaic_512/0 [0]:│    95 │   │   │   if full_key:                                                                   │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│ /opt/conda/lib/python3.9/site-packages/composer/trainer/trainer.py:1330 in __init__              │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│   1327 │   │   │   │   if wandb.run is None:                                                     │
mosaic_512/0 [0]:│   1328 │   │   │   │   │   load_object_store.init(self.state, self.logger)                       │
mosaic_512/0 [0]:│   1329 │   │   │   _, _, parsed_load_path = parse_uri(load_path)                                 │
mosaic_512/0 [0]:│ ❱ 1330 │   │   │   self._rng_state = checkpoint.load_checkpoint(                                 │
mosaic_512/0 [0]:│   1331 │   │   │   │   state=self.state,                                                         │
mosaic_512/0 [0]:│   1332 │   │   │   │   logger=self.logger,                                                       │
mosaic_512/0 [0]:│   1333 │   │   │   │   path=parsed_load_path,                                                    │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│ /opt/conda/lib/python3.9/site-packages/composer/utils/checkpoint.py:205 in load_checkpoint       │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│   202 │   │   │   # Get the path to the proper checkpoint folder corresponding to the current    │
mosaic_512/0 [0]:│   203 │   │   │   # If fsdp_sharded_state_dict_enabled then just use that rank's unique tempdi   │
mosaic_512/0 [0]:│   204 │   │   │   node_checkpoint_folder = (tempdir                                              │
mosaic_512/0 [0]:│ ❱ 205 │   │   │   │   │   │   │   │   │     if state.fsdp_sharded_state_dict_enabled else _get   │
mosaic_512/0 [0]:│   206 │   │   │   assert node_checkpoint_folder is not None                                      │
mosaic_512/0 [0]:│   207 │   │   │                                                                                  │
mosaic_512/0 [0]:│   208 │   │   │   composer_states_filepath, extracted_checkpoint_folder, extracted_rank_n = do   │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│ /opt/conda/lib/python3.9/site-packages/composer/utils/checkpoint.py:239 in                       │
mosaic_512/0 [0]:│ _get_local_rank_zero_path                                                                        │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│   236                                                                                            │
mosaic_512/0 [0]:│   237 def _get_local_rank_zero_path(path: Optional[str]) -> str:                                 │
mosaic_512/0 [0]:│   238 │   """Broadcasts the ``path`` from the LOCAL rank zero to all LOCAL ranks."""             │
mosaic_512/0 [0]:│ ❱ 239 │   local_rank_zero = dist.get_local_world_size() * dist.get_node_rank()                   │
mosaic_512/0 [0]:│   240 │   paths = dist.all_gather_object(path)                                                   │
mosaic_512/0 [0]:│   241 │   local_rank_zero_path = paths[local_rank_zero]                                          │
mosaic_512/0 [0]:│   242 │   assert local_rank_zero_path is not None, 'local rank zero provides the path'           │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│ /opt/conda/lib/python3.9/site-packages/composer/utils/dist.py:155 in get_node_rank               │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│   152 │   Returns:                                                                               │
mosaic_512/0 [0]:│   153 │   │   int: The node rank, starting at 0.                                                 │
mosaic_512/0 [0]:│   154 │   """                                                                                    │
mosaic_512/0 [0]:│ ❱ 155 │   return _get_distributed_config_var(env_var='NODE_RANK', default=0, human_name='node    │
mosaic_512/0 [0]:│   156                                                                                            │
mosaic_512/0 [0]:│   157                                                                                            │
mosaic_512/0 [0]:│   158 def barrier() -> None:                                                                     │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│ /opt/conda/lib/python3.9/site-packages/composer/utils/dist.py:101 in _get_distributed_config_var │
mosaic_512/0 [0]:│                                                                                                  │
mosaic_512/0 [0]:│    98 │   │   return int(os.environ[env_var])                                                    │
mosaic_512/0 [0]:│    99 │                                                                                          │
mosaic_512/0 [0]:│   100 │   if dist.is_initialized():                                                              │
mosaic_512/0 [0]:│ ❱ 101 │   │   raise MissingEnvironmentError('Torch distributed is initialized but environment    │
mosaic_512/0 [0]:│   102 │   │   │   │   │   │   │   │   │     f'{env_var} is not set.')                            │
mosaic_512/0 [0]:│   103 │                                                                                          │
mosaic_512/0 [0]:│   104 │   return default                                                                         │
mosaic_512/0 [0]:╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
mosaic_512/0 [0]:MissingEnvironmentError: Torch distributed is initialized but environment variable NODE_RANK is not set.

BTW, I am using torchx to run your code.
torchx run -s local_cwd spmd -j 1x1 --script mosaic_512.py

Answer:

We normally run this with the composer launcher as referenced in the readme:

composer run.py --config-path yamls/hydra-yamls --config-name SD-2-base-256.yaml

I haven't used torchx, but it seems like you will have to set some distributed env variablers on your own. For reference, here are the variables the composer launcher handles for you.
https://github.com/mosaicml/composer/blob/dev/composer/utils/dist.py

To be clear, torchx is not something we currently support. If it works, that's great! but we're likely going to be of limited use debugging it.
If the composer commands work for you. That's likely the easier path forward. If there's a problem with those commands, we can certainly help.