base_config:
  integrations:
    - integration_type: git_repo
      git_repo: mosaicml/examples
      git_branch: feature/composer_icl_eval
      pip_install: -r llm/requirements.txt

  # We are fetching, converting, and training on the 'val' split
  # as it is small and quick to get going for this demo.
  # For real training runs, follow the instructions in `examples/llm/README.md`
  # to convert and host the full 'train' dataset.
  command: |
    cd examples/llm
    composer -m icl_eval.evaluate_model /mnt/config/parameters.yaml 

  image: mosaicml/pytorch:1.13.0_cu117-python3.10-ubuntu20.04
  optimization_level: 0

  run_name: mosaic-gpt-1b-eval
  gpu_num: 8
  gpu_type: a100_40gb
  cluster: r7z2 # replace with your cluster here!


  # The below is injected as a YAML file: /mnt/config/parameters.yaml
  # but is not used in this example.
  parameters:
    tokenizer:
      type: hftokenizer
      args:
        tokenizer_name: gpt2
        max_seq_len: 2048  
    model: {}
    icl_tasks: {}


sweep_config:
  gpu_num: 
    - 
      key: '256'
      value: 256
    - 
      key: '128'
      value: 128
    - 
      key: '64'
      value: 64
    - 
      key: '32'
      value: 32
    - 
      key: '16'
      value: 16
    -
      key: '8'
      value: 8
    
 
  parameters.icl_tasks:
    # -
    #   key:  lambada
    #   value:
    #     -
    #       label: lambada
    #       dataset_uri: s3://mosaicml-internal-dataset-lambda/lambada/lambada_openai.jsonl
    #       num_fewshot: 
    #           - 0
    #       batch_size: 4
    #       type: language_modeling
    #       metrics:
    #           - InContextLearningLMAccuracy
    #       formatting_options:
    #         prompt_string: ""
    #         example_delimiter: "\n"
    #         continuation_delimiter: ""
    -
      key:  piqa
      value:  
        -
          label: piqa
          dataset_uri: s3://mosaicml-internal-dataset-hellaswag/piqa.jsonz
          num_fewshot: 
              - 5
          batch_size: 4
          type: multiple_choice
          metrics:
              - InContextLearningMultipleChoiceAccuracy
          formatting_options:
            prompt_string: ""
            example_delimiter: "\n"
            continuation_delimiter: " "
  
  parameters.model:
    -
      key: '125m'
      value:
        model_type: pretrained_hf
        config: EleutherAI/gpt-neo-125M
    -
      key: '1.3b'
      value:
        model_type: pretrained_hf
        config: EleutherAI/gpt-neo-1.3B
    -
      key: '2.7b'
      value:
        model_type: pretrained_hf
        config: EleutherAI/gpt-neo-2.7B