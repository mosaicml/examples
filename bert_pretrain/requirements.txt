datasets<3
einops==0.5.0
mosaicml==0.11.0
mosaicml-streaming<1
omegaconf<3
transformers<5
torchvision<0.14
torchtext<0.14
torch<1.13
wandb<1

# fork of flash attention which supports attention bias
# will not install if no CUDA device
# this branch seems to have important fixes for multi-gpu
flash_attn @ git+https://github.com/dptech-corp/flash-attention.git@fix-conflict
